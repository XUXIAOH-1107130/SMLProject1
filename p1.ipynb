{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c8f674-1e29-4543-a51e-e2ebf9ea4098",
   "metadata": {},
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be6ade1f-59fe-4fe6-a5b2-870aba3e2154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text  label\n",
      "0   0  16 231 543 5 15 43 8282 94 231 1129 31 34 32 9...      1\n",
      "1   1  16 4046 138 10 2 1809 2007 3763 14 40113 13 90...      1\n",
      "2   2  1108 16550 3 6168 3 160 284 19 49 464 5333 8 4...      1\n",
      "3   3  1802 27 16 25 48 451 632 3 2 2164 25 2380 34 7...      1\n",
      "4   4  16 19 302 93 97 43 952 118 1 16 528 2 26528 10...      1\n",
      "     id                                               text  label\n",
      "0  5000  12 920 7 1266 28 9884 1640 116 11 1342 1533 28...      1\n",
      "1  5001  783 397 253 5797 9379 22 793 11838 10 607 6324...      1\n",
      "2  5002  888 14851 323 9 27 1377 584 195 3 137 10 2732 ...      1\n",
      "3  5003  228 1161 5815 379 9 941 10 2 316 4 2693 594 87...      1\n",
      "4  5004  736 19 37 813 45 6723 27 626 8 2 3446 4 564 34...      1\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "#**Preprocessing**\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_domain_from_json(path):\n",
    "    domain = []\n",
    "    with open(path, \"r\") as file:\n",
    "        for instance in file:\n",
    "            domain.append(json.loads(instance))\n",
    "    # print(domain[0])\n",
    "\n",
    "    domain_label = [instance[\"label\"] for instance in domain]\n",
    "    domain_id = [instance[\"id\"] for instance in domain]\n",
    "    domain_text = [instance[\"text\"] for instance in domain]\n",
    "    for i in range(len(domain_text)):\n",
    "        text = list(map(str, domain_text[i]))\n",
    "        domain_text[i] = \" \".join(text)\n",
    "    # print(domain_text[0])\n",
    "    domain_df = pd.DataFrame({\n",
    "        \"id\": domain_id,\n",
    "        \"text\": domain_text,\n",
    "        \"label\": domain_label\n",
    "    })\n",
    "    return domain_df\n",
    "\n",
    "domain1 = load_domain_from_json(\"data/domain1_train_data.json\")\n",
    "domain2 = load_domain_from_json(\"data/domain2_train_data.json\")\n",
    "print(domain1.head())\n",
    "print(domain2.head())\n",
    "\n",
    "test_data = []\n",
    "with open(\"data/test_data.json\", \"r\") as file:\n",
    "    for instance in file:\n",
    "        test_data.append(json.loads(instance))\n",
    "id = [instance[\"id\"] for instance in test_data]\n",
    "text = [instance[\"text\"] for instance in test_data]\n",
    "for i in range(len(text)):\n",
    "    text_str = list(map(str, text[i]))\n",
    "    text[i] = \" \".join(text_str)\n",
    "test_data_df = pd.DataFrame({\n",
    "    \"id\": id,\n",
    "    \"text\": text,\n",
    "})\n",
    "# print(test_data_df.head())\n",
    "print(len(test_data_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592dbdc2-8159-47aa-8ad1-c34a904db1e5",
   "metadata": {},
   "source": [
    "## **Baseline(BOW + NaiveBayes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a439e3-4485-49f8-9ad4-55833f54c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.6188888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "combined_domain = pd.concat([domain1, domain2], ignore_index=True)\n",
    "vectorizer_BOW = CountVectorizer()\n",
    "X = combined_domain['text']\n",
    "y = combined_domain['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train = vectorizer_BOW.fit_transform(X_train)\n",
    "X_test = vectorizer_BOW.transform(X_test)\n",
    "\n",
    "nb_classifier = BernoulliNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Baseline Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb9dc4-766b-48bd-a4aa-f178c3d63c3d",
   "metadata": {},
   "source": [
    "# **Undersampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48cbc25-a9d5-4b1b-b13c-3ead87588161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1500\n",
      "1    1500\n",
      "Name: label, dtype: int64\n",
      "Accuracy: 0.713125\n",
      "Accuracy: 0.633125\n",
      "SVM Accuracy: 0.778125\n",
      "Logistic Regression Accuracy: 0.74625\n",
      "XGBoost Accuracy: 0.73125\n",
      "Random Forest Accuracy: 0.780625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# **Undersampling + BOW**\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "label_counts = domain2['label'].value_counts()\n",
    "majority_label = label_counts[label_counts == label_counts.max()].index[0]\n",
    "minority_label = label_counts[label_counts == label_counts.min()].index[0]\n",
    "\n",
    "domain2_majority = domain2[domain2['label'] == majority_label]\n",
    "domain2_minority = domain2[domain2['label'] == minority_label]\n",
    "\n",
    "domain2_majority_underampled = resample(domain2_majority,\n",
    "                                        replace=False,\n",
    "                                        n_samples=len(domain2_minority),\n",
    "                                        random_state=42)\n",
    "\n",
    "domain2_undersampled = pd.concat([domain2_majority_underampled, domain2_minority])\n",
    "\n",
    "print(domain2_undersampled['label'].value_counts())\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "combined_data = pd.concat([domain1, domain2_undersampled], ignore_index=True)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(combined_data['text'])\n",
    "y = combined_data['label']\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mnb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "## **Undersampling + TFIDF**\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(combined_data['text'])\n",
    "y = combined_data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# more models\n",
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)\n",
    "\n",
    "#LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_classifier.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
    "\n",
    "#XGB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"XGBoost Accuracy:\", accuracy_xgb)\n",
    "\n",
    "#RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561ae30-4eef-4a52-bda1-ec538c572d06",
   "metadata": {},
   "source": [
    "## **Oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd89c445-b908-43f2-8681-ef5ac5aac9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 73066)\n",
      "             BOW     TFIDF\n",
      "SMOTE   0.588611  0.776944\n",
      "ADASYN  0.583611  0.776944\n",
      "SVM Accuracy: 0.8591666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### SMOTE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "combined_domain = pd.concat([domain1, domain2], ignore_index=True)\n",
    "# print(combined_domain)\n",
    "accuracy_df = pd.DataFrame(index=['SMOTE', 'ADASYN'], columns=['BOW', 'TFIDF'])\n",
    "\n",
    "########### BOW\n",
    "vectorizer_BOW = CountVectorizer()\n",
    "X = combined_domain['text']\n",
    "y = combined_domain['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_BOW = vectorizer_BOW.fit_transform(X_train)\n",
    "X_test_BOW = vectorizer_BOW.transform(X_test)\n",
    "\n",
    "X_train_BOW_resampled_SMOTE, y_train_BOW_resampled_SMOTE = SMOTE(sampling_strategy='auto', random_state=35, k_neighbors=5, n_jobs=None).fit_resample(X_train_BOW, y_train)\n",
    "# print(X_train_resampled.shape)\n",
    "# print(y_train_resampled.shape)\n",
    "\n",
    "nb_classifier = BernoulliNB()\n",
    "nb_classifier.fit(X_train_BOW_resampled_SMOTE, y_train_BOW_resampled_SMOTE)\n",
    "\n",
    "y_pred_BOW_SMOTE = nb_classifier.predict(X_test_BOW)\n",
    "accuracy = accuracy_score(y_test, y_pred_BOW_SMOTE)\n",
    "accuracy_df.loc['SMOTE', 'BOW'] = accuracy\n",
    "\n",
    "########## TFIDF\n",
    "\n",
    "vectorizer_TFIDF = TfidfVectorizer()\n",
    "X_train_TFIDF = vectorizer_TFIDF.fit_transform(X_train)\n",
    "X_test_TFIDF = vectorizer_TFIDF.transform(X_test)\n",
    "x = test_data_df[\"text\"]\n",
    "assert(len(x)==4000)\n",
    "X_test_oversampling = vectorizer_TFIDF.transform(x)\n",
    "print(X_test_oversampling.shape)\n",
    "\n",
    "X_train_TFIDF_resampled_SMOTE, y_train_TFIDF_resampled_SMOTE = SMOTE(sampling_strategy='auto', random_state=35, k_neighbors=5, n_jobs=None).fit_resample(X_train_TFIDF, y_train)\n",
    "# print(X_train_resampled.shape)\n",
    "# print(y_train_resampled.shape)\n",
    "nb_classifier = BernoulliNB()\n",
    "nb_classifier.fit(X_train_TFIDF_resampled_SMOTE, y_train_TFIDF_resampled_SMOTE)\n",
    "\n",
    "y_pred_TFIDF_SMOTE = nb_classifier.predict(X_test_TFIDF)\n",
    "accuracy = accuracy_score(y_test, y_pred_TFIDF_SMOTE)\n",
    "accuracy_df.loc['SMOTE', 'TFIDF'] = accuracy\n",
    "\n",
    "\n",
    "### ADASYN\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "X_train_TFIDF_resampled_ADA, y_train_TFIDF_resampled_ADA = ADASYN(sampling_strategy='auto', random_state=35, n_neighbors=5, n_jobs=None).fit_resample(X_train_TFIDF, y_train)\n",
    "# print(X_train_resampled.shape)\n",
    "# print(y_train_resampled.shape)\n",
    "\n",
    "nb_classifier = BernoulliNB()\n",
    "nb_classifier.fit(X_train_TFIDF_resampled_ADA, y_train_TFIDF_resampled_ADA)\n",
    "\n",
    "y_pred_TFIDF_ADA = nb_classifier.predict(X_test_TFIDF)\n",
    "accuracy = accuracy_score(y_test, y_pred_TFIDF_ADA)\n",
    "accuracy_df.loc['ADASYN', 'TFIDF'] = accuracy\n",
    "\n",
    "\n",
    "X_train_BOW_resampled_ADA, y_train_BOW_resampled_ADA = ADASYN(sampling_strategy='auto', random_state=35, n_neighbors=5, n_jobs=None).fit_resample(X_train_BOW, y_train)\n",
    "# print(X_train_resampled.shape)\n",
    "# print(y_train_resampled.shape)\n",
    "\n",
    "nb_classifier = BernoulliNB()\n",
    "nb_classifier.fit(X_train_BOW_resampled_ADA, y_train_BOW_resampled_ADA)\n",
    "\n",
    "y_pred_BOW_ADA = nb_classifier.predict(X_test_BOW)\n",
    "accuracy = accuracy_score(y_test, y_pred_BOW_ADA)\n",
    "accuracy_df.loc['ADASYN', 'BOW'] = accuracy\n",
    "\n",
    "print(accuracy_df)\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train_TFIDF_resampled_ADA, y_train_TFIDF_resampled_ADA)\n",
    "\n",
    "y_pred_svm_ADA = svm_classifier.predict(X_test_TFIDF)\n",
    "accuracy_svm_ADA = accuracy_score(y_test, y_pred_svm_ADA)\n",
    "print(\"SVM Accuracy:\", accuracy_svm_ADA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95609da3-2bac-42eb-be2f-e4eac4ca66d1",
   "metadata": {},
   "source": [
    "## **Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d909a247-5e68-48d1-8623-069457cb4a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences_list = []\n",
    "text_column = domain1['text']\n",
    "for line in text_column:\n",
    "    sentences_list.append(nltk.word_tokenize(line))\n",
    "print(sentences_list)\n",
    "\n",
    "num_features = 300    \n",
    "min_word_count = 40   \n",
    "num_workers = 4       \n",
    "context = 10          \n",
    "model_name = 'word2vec1'\n",
    "\n",
    "model = Word2Vec(sentences_list, workers=num_workers, vector_size=num_features, min_count=min_word_count, window=context)\n",
    "\n",
    "model_directory = '..\\\\models'\n",
    "model_filename = 'word2vec1'\n",
    "full_path = os.path.join(model_directory, model_filename)\n",
    "\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "model.save(full_path)\n",
    "print(f\"Model will be saved to: {full_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390e4f7-e77c-4c6a-a57e-356268e9d5c6",
   "metadata": {},
   "source": [
    "## **DaNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e038ec2-713a-43c3-af2e-5d995787eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self, feature_dim=len(vectorizer.vocabulary_)):\n",
    "        super(Extractor, self).__init__()\n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Linear(in_features=feature_dim, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.extractor(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=1),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_feature, alpha):\n",
    "        reversed_input = ReverseLayerF.apply(input_feature, alpha)\n",
    "        x = self.discriminator(reversed_input)\n",
    "        return x\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05388a89-2194-4634-a5de-77ff894a45f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Total Loss: 42.52740478515625, Label Loss: 41.89863586425781, Domain Loss: 0.6287685632705688\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(domain1['text'], domain1['label'].values, test_size=0.2, random_state=42, stratify=domain1['label'].values)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(domain2['text'], domain2['label'].values, test_size=0.2, random_state=42, stratify=domain2['label'].values)\n",
    "\n",
    "combined_X_train = pd.concat([X_train_1, X_train_2])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(combined_X_train)\n",
    "\n",
    "X_train_1_TFIDF = vectorizer.transform(X_train_1)\n",
    "X_train_2_TFIDF = vectorizer.transform(X_train_2)\n",
    "\n",
    "X_train_2_TFIDF, y_train_2 = ADASYN(sampling_strategy='auto', random_state=35, n_neighbors=5, n_jobs=None).fit_resample(X_train_2_TFIDF, y_train_2)\n",
    "\n",
    "X_train_1 = torch.from_numpy(X_train_1_TFIDF.toarray())\n",
    "y_train_1 = torch.from_numpy(y_train_1).to(dtype=torch.float)\n",
    "X_train_2 = torch.from_numpy(X_train_2_TFIDF.toarray())\n",
    "y_train_2 = torch.from_numpy(y_train_2).to(dtype=torch.float)\n",
    "\n",
    "# Create iterable dataset in Torch format\n",
    "train_1_ds = torch.utils.data.TensorDataset(X_train_1, y_train_1)\n",
    "train_1_loader = torch.utils.data.DataLoader(train_1_ds, batch_size=32)\n",
    "train_2_ds = torch.utils.data.TensorDataset(X_train_2, y_train_2)\n",
    "train_2_loader = torch.utils.data.DataLoader(train_2_ds, batch_size=32)\n",
    "\n",
    "\n",
    "feature_extractor = Extractor(feature_dim=len(vectorizer.vocabulary_))\n",
    "label_classifier = Classifier()\n",
    "domain_classifier = Discriminator()\n",
    "\n",
    "label_classification_criterion = nn.CrossEntropyLoss()\n",
    "domain_classification_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "DaNN_params = list(feature_extractor.parameters()) + list(label_classifier.parameters()) + list(domain_classifier.parameters())\n",
    "DaNN_optimizer = optim.Adam(DaNN_params, lr=0.001)\n",
    "\n",
    "alpha = 0.5\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    feature_extractor.train()\n",
    "    label_classifier.train()\n",
    "    domain_classifier.train()\n",
    "    for i, (data_1, data_2) in enumerate(zip(train_1_loader, train_2_loader)):\n",
    "        text_1, label_1 = data_1\n",
    "        text_2, label_2 = data_2\n",
    "        combined_text = torch.cat((text_1, text_2), 0)\n",
    "        \n",
    "        feature_1 = feature_extractor(text_1)\n",
    "        feature_2 = feature_extractor(text_2)\n",
    "        combined_feature = feature_extractor(combined_text)\n",
    "        \n",
    "        label_prediction_1 = label_classifier(feature_1)\n",
    "        label_prediction_2 = label_classifier(feature_2)\n",
    "        # print(label_prediction_1)\n",
    "        # print(label_1)\n",
    "        label_loss_1 = label_classification_criterion(label_prediction_1, label_1)\n",
    "        label_loss_2 = label_classification_criterion(label_prediction_2, label_2)\n",
    "\n",
    "        domain_prediction = domain_classifier(combined_feature, alpha)\n",
    "        domain_combined_label = torch.cat((torch.zeros(text_1.shape[0]).long(), torch.ones(text_2.shape[0]).long()), 0)\n",
    "\n",
    "        domain_loss = domain_classification_criterion(domain_prediction, domain_combined_label)\n",
    "        \n",
    "        total_loss = label_loss_1 + label_loss_2 + domain_loss\n",
    "        DaNN_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        DaNN_optimizer.step()\n",
    "        \n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}, Total Loss: {total_loss.item()}, Label Loss: {(label_loss_1+label_loss_2).item()}, Domain Loss: {domain_loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5babe3-5c00-4efb-abc4-439ffb5b8187",
   "metadata": {},
   "source": [
    "## **Test Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4266a2b-c8fc-4f6d-853d-4a1da4ea8eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  class\n",
      "0        0      1\n",
      "1        1      0\n",
      "2        2      0\n",
      "3        3      0\n",
      "4        4      0\n",
      "...    ...    ...\n",
      "3995  3995      0\n",
      "3996  3996      0\n",
      "3997  3997      0\n",
      "3998  3998      0\n",
      "3999  3999      1\n",
      "\n",
      "[4000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_predict_csv(clfs, X_test, id):\n",
    "    \n",
    "    prediction = clfs.predict(X_test)\n",
    "    results_csv = pd.DataFrame({\n",
    "        \"id\": id,\n",
    "        \"class\": prediction,\n",
    "    })\n",
    "    return results_csv\n",
    "\n",
    "test_results = get_predict_csv(svm_classifier, X_test_oversampling, test_data_df[\"id\"])\n",
    "print(test_results)\n",
    "test_results.to_csv('test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff847852-3b79-41a4-808f-e52b561736db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "839bb10b-6318-471a-bcff-fae1655867da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10, Total Loss: 108765.1796875, Label Loss: 108764.5390625, Domain Loss: 0.6370952129364014\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20, Total Loss: 104948.5234375, Label Loss: 104947.953125, Domain Loss: 0.573634684085846\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30, Total Loss: 102620.3515625, Label Loss: 102619.859375, Domain Loss: 0.49597883224487305\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40, Total Loss: 102241.40625, Label Loss: 102240.921875, Domain Loss: 0.4878908395767212\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m label_loss_1 \u001b[38;5;241m+\u001b[39m label_loss_2 \u001b[38;5;241m+\u001b[39m domain_loss\n\u001b[1;32m     64\u001b[0m DaNN_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 65\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     66\u001b[0m DaNN_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(domain1['text'], domain1['label'].values, test_size=0.2, random_state=42, stratify=domain1['label'].values)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(domain2['text'], domain2['label'].values, test_size=0.2, random_state=42, stratify=domain2['label'].values)\n",
    "\n",
    "combined_X_train = pd.concat([X_train_1, X_train_2])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(combined_X_train)\n",
    "\n",
    "X_train_1_TFIDF = vectorizer.transform(X_train_1)\n",
    "X_train_2_TFIDF = vectorizer.transform(X_train_2)\n",
    "\n",
    "X_train_2_TFIDF, y_train_2 = ADASYN(sampling_strategy='auto', random_state=35, n_neighbors=5, n_jobs=None).fit_resample(X_train_2_TFIDF, y_train_2)\n",
    "\n",
    "X_train_1 = torch.from_numpy(X_train_1_TFIDF.toarray()).to(dtype=torch.float)\n",
    "y_train_1 = torch.from_numpy(y_train_1).to(dtype=torch.float)\n",
    "X_train_2 = torch.from_numpy(X_train_2_TFIDF.toarray()).to(dtype=torch.float)\n",
    "y_train_2 = torch.from_numpy(y_train_2).to(dtype=torch.float)\n",
    "\n",
    "# Create iterable dataset in Torch format\n",
    "train_1_ds = torch.utils.data.TensorDataset(X_train_1, y_train_1)\n",
    "train_1_loader = torch.utils.data.DataLoader(train_1_ds, batch_size=32)\n",
    "train_2_ds = torch.utils.data.TensorDataset(X_train_2, y_train_2)\n",
    "train_2_loader = torch.utils.data.DataLoader(train_2_ds, batch_size=32)\n",
    "\n",
    "\n",
    "feature_extractor = Extractor(feature_dim=len(vectorizer.vocabulary_))\n",
    "label_classifier = Classifier()\n",
    "domain_classifier = Discriminator()\n",
    "\n",
    "label_classification_criterion = nn.CrossEntropyLoss()\n",
    "domain_classification_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "DaNN_params = list(feature_extractor.parameters()) + list(label_classifier.parameters()) + list(domain_classifier.parameters())\n",
    "DaNN_optimizer = optim.Adam(DaNN_params, lr=0.001)\n",
    "\n",
    "alpha = 0.5\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    \n",
    "    feature_extractor.train()\n",
    "    label_classifier.train()\n",
    "    domain_classifier.train()\n",
    "\n",
    "    feature_1 = feature_extractor(X_train_1)\n",
    "    feature_2 = feature_extractor(X_train_2)\n",
    "    X_train_combined = torch.cat((X_train_1, X_train_2), 0)\n",
    "    combined_feature = feature_extractor(X_train_combined)\n",
    "\n",
    "    label_prediction_1 = label_classifier(feature_1)\n",
    "    label_prediction_2 = label_classifier(feature_2)\n",
    "    label_loss_1 = label_classification_criterion(label_prediction_1, y_train_1)\n",
    "    label_loss_2 = label_classification_criterion(label_prediction_2, y_train_2)\n",
    "    \n",
    "    domain_prediction = domain_classifier(combined_feature, alpha)\n",
    "    domain_combined_label = torch.cat((torch.zeros(X_train_1.shape[0]).long(), torch.ones(X_train_2.shape[0]).long()), 0)\n",
    "    domain_loss = domain_classification_criterion(domain_prediction, domain_combined_label)\n",
    "        \n",
    "    total_loss = label_loss_1 + label_loss_2 + domain_loss\n",
    "    DaNN_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    DaNN_optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch: {epoch+1}, Total Loss: {total_loss.item()}, Label Loss: {(label_loss_1+label_loss_2).item()}, Domain Loss: {domain_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306c7c3-1b56-4aaf-ba3f-49133b85fbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
