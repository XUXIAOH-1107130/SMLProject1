{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c8f674-1e29-4543-a51e-e2ebf9ea4098",
   "metadata": {},
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be6ade1f-59fe-4fe6-a5b2-870aba3e2154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text  label\n",
      "0   0  16 231 543 5 15 43 8282 94 231 1129 31 34 32 9...      1\n",
      "1   1  16 4046 138 10 2 1809 2007 3763 14 40113 13 90...      1\n",
      "2   2  1108 16550 3 6168 3 160 284 19 49 464 5333 8 4...      1\n",
      "3   3  1802 27 16 25 48 451 632 3 2 2164 25 2380 34 7...      1\n",
      "4   4  16 19 302 93 97 43 952 118 1 16 528 2 26528 10...      1\n",
      "     id                                               text  label\n",
      "0  5000  12 920 7 1266 28 9884 1640 116 11 1342 1533 28...      1\n",
      "1  5001  783 397 253 5797 9379 22 793 11838 10 607 6324...      1\n",
      "2  5002  888 14851 323 9 27 1377 584 195 3 137 10 2732 ...      1\n",
      "3  5003  228 1161 5815 379 9 941 10 2 316 4 2693 594 87...      1\n",
      "4  5004  736 19 37 813 45 6723 27 626 8 2 3446 4 564 34...      1\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "#**Preprocessing**\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_domain_from_json(path):\n",
    "    domain = []\n",
    "    with open(path, \"r\") as file:\n",
    "        for instance in file:\n",
    "            domain.append(json.loads(instance))\n",
    "    # print(domain[0])\n",
    "\n",
    "    domain_label = [instance[\"label\"] for instance in domain]\n",
    "    domain_id = [instance[\"id\"] for instance in domain]\n",
    "    domain_text = [instance[\"text\"] for instance in domain]\n",
    "    for i in range(len(domain_text)):\n",
    "        text = list(map(str, domain_text[i]))\n",
    "        domain_text[i] = \" \".join(text)\n",
    "    # print(domain_text[0])\n",
    "    domain_df = pd.DataFrame({\n",
    "        \"id\": domain_id,\n",
    "        \"text\": domain_text,\n",
    "        \"label\": domain_label\n",
    "    })\n",
    "    return domain_df\n",
    "\n",
    "domain1 = load_domain_from_json(\"data/domain1_train_data.json\")\n",
    "domain2 = load_domain_from_json(\"data/domain2_train_data.json\")\n",
    "print(domain1.head())\n",
    "print(domain2.head())\n",
    "\n",
    "test_data = []\n",
    "with open(\"data/test_data.json\", \"r\") as file:\n",
    "    for instance in file:\n",
    "        test_data.append(json.loads(instance))\n",
    "id = [instance[\"id\"] for instance in test_data]\n",
    "text = [instance[\"text\"] for instance in test_data]\n",
    "for i in range(len(text)):\n",
    "    text_str = list(map(str, text[i]))\n",
    "    text[i] = \" \".join(text_str)\n",
    "test_data_df = pd.DataFrame({\n",
    "    \"id\": id,\n",
    "    \"text\": text,\n",
    "})\n",
    "# print(test_data_df.head())\n",
    "print(len(test_data_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592dbdc2-8159-47aa-8ad1-c34a904db1e5",
   "metadata": {},
   "source": [
    "## **Baseline(BOW + NaiveBayes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a439e3-4485-49f8-9ad4-55833f54c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.6188888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "combined_domain = pd.concat([domain1, domain2], ignore_index=True)\n",
    "vectorizer_BOW = CountVectorizer()\n",
    "X = combined_domain['text']\n",
    "y = combined_domain['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train = vectorizer_BOW.fit_transform(X_train)\n",
    "X_test = vectorizer_BOW.transform(X_test)\n",
    "\n",
    "nb_classifier = BernoulliNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Baseline Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb9dc4-766b-48bd-a4aa-f178c3d63c3d",
   "metadata": {},
   "source": [
    "# **Undersampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48cbc25-a9d5-4b1b-b13c-3ead87588161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1500\n",
      "1    1500\n",
      "Name: label, dtype: int64\n",
      "Accuracy: 0.713125\n",
      "Accuracy: 0.633125\n",
      "SVM Accuracy: 0.778125\n",
      "Logistic Regression Accuracy: 0.74625\n",
      "XGBoost Accuracy: 0.73125\n",
      "Random Forest Accuracy: 0.780625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# **Undersampling + BOW**\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "label_counts = domain2['label'].value_counts()\n",
    "majority_label = label_counts[label_counts == label_counts.max()].index[0]\n",
    "minority_label = label_counts[label_counts == label_counts.min()].index[0]\n",
    "\n",
    "domain2_majority = domain2[domain2['label'] == majority_label]\n",
    "domain2_minority = domain2[domain2['label'] == minority_label]\n",
    "\n",
    "domain2_majority_underampled = resample(domain2_majority,\n",
    "                                        replace=False,\n",
    "                                        n_samples=len(domain2_minority),\n",
    "                                        random_state=42)\n",
    "\n",
    "domain2_undersampled = pd.concat([domain2_majority_underampled, domain2_minority])\n",
    "\n",
    "print(domain2_undersampled['label'].value_counts())\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "combined_data = pd.concat([domain1, domain2_undersampled], ignore_index=True)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(combined_data['text'])\n",
    "y = combined_data['label']\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mnb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "## **Undersampling + TFIDF**\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(combined_data['text'])\n",
    "y = combined_data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# more models\n",
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)\n",
    "\n",
    "#LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_classifier.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
    "\n",
    "#XGB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"XGBoost Accuracy:\", accuracy_xgb)\n",
    "\n",
    "#RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561ae30-4eef-4a52-bda1-ec538c572d06",
   "metadata": {},
   "source": [
    "## **Oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd89c445-b908-43f2-8681-ef5ac5aac9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 73066)\n",
      "             BOW     TFIDF\n",
      "SMOTE   0.588611  0.776944\n",
      "ADASYN  0.583611  0.776944\n",
      "SVM Accuracy: 0.8591666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### SMOTE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "combined_domain = pd.concat([domain1, domain2], ignore_index=True)\n",
    "# print(combined_domain)\n",
    "accuracy_df = pd.DataFrame(index=['SMOTE', 'ADASYN'], columns=['BOW', 'TFIDF'])\n",
    "\n",
    "########### BOW\n",
    "vectorizer_BOW = CountVectorizer()\n",
    "X = combined_domain['text']\n",
    "y = combined_domain['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_BOW = vectorizer_BOW.fit_transform(X_train)\n",
    "X_test_BOW = vectorizer_BOW.transform(X_test)\n",
    "\n",
    "X_train_BOW_resampled_SMOTE, y_train_BOW_resampled_SMOTE = SMOTE(sampling_strategy='auto', random_state=35, k_neighbors=5, n_jobs=None).fit_resample(X_train_BOW, y_train)\n",
    "# print(X_train_resampled.shape)\n",
    "# print(y_train_resampled.shape)\n",
    "\n",
    "nb_classifier = BernoulliNB()\n",
    "nb_classifier.fit(X_train_BOW_resampled_SMOTE, y_train_BOW_resampled_SMOTE)\n",
    "\n",
    "y_pred_BOW_SMOTE = nb_classifier.predict(X_test_BOW)\n",
    "accuracy = accuracy_score(y_test, y_pred_BOW_SMOTE)\n",
    "accuracy_df.loc['SMOTE', 'BOW'] = accuracy\n",
    "\n",
    "########## TFIDF\n",
    "\n",
    "vectorizer_TFIDF = TfidfVectorizer()\n",
    "X_train_TFIDF = vectorizer_TFIDF.fit_transform(X_train)\n",
    "X_test_TFIDF = vectorizer_TFIDF.transform(X_test)\n",
    "x = test_data_df[\"text\"]\n",
    "assert(len(x)==4000)\n",
    "X_test_oversampling = vectorizer_TFIDF.transform(x)\n",
    "print(X_test_oversampling.shape)\n",
    "\n",
    "X_train_TFIDF_resampled_SMOTE, y_train_TFIDF_resampled_SMOTE = SMOTE(sampling_strategy='auto', random_state=35, k_neighbors=5, n_jobs=None).fit_resample(X_train_TFIDF, y_train)\n",
    "# print(X_train_resampled.shape)\n",
    "# print(y_train_resampled.shape)\n",
    "nb_classifier = BernoulliNB()\n",
    "nb_classifier.fit(X_train_TFIDF_resampled_SMOTE, y_train_TFIDF_resampled_SMOTE)\n",
    "\n",
    "y_pred_TFIDF_SMOTE = nb_classifier.predict(X_test_TFIDF)\n",
    "accuracy = accuracy_score(y_test, y_pred_TFIDF_SMOTE)\n",
    "accuracy_df.loc['SMOTE', 'TFIDF'] = accuracy\n",
    "\n",
    "\n",
    "### ADASYN\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "X_train_TFIDF_resampled_ADA, y_train_TFIDF_resampled_ADA = ADASYN(sampling_strategy='auto', random_state=35, n_neighbors=5, n_jobs=None).fit_resample(X_train_TFIDF, y_train)\n",
    "# print(X_train_resampled.shape)\n",
    "# print(y_train_resampled.shape)\n",
    "\n",
    "nb_classifier = BernoulliNB()\n",
    "nb_classifier.fit(X_train_TFIDF_resampled_ADA, y_train_TFIDF_resampled_ADA)\n",
    "\n",
    "y_pred_TFIDF_ADA = nb_classifier.predict(X_test_TFIDF)\n",
    "accuracy = accuracy_score(y_test, y_pred_TFIDF_ADA)\n",
    "accuracy_df.loc['ADASYN', 'TFIDF'] = accuracy\n",
    "\n",
    "\n",
    "X_train_BOW_resampled_ADA, y_train_BOW_resampled_ADA = ADASYN(sampling_strategy='auto', random_state=35, n_neighbors=5, n_jobs=None).fit_resample(X_train_BOW, y_train)\n",
    "# print(X_train_resampled.shape)\n",
    "# print(y_train_resampled.shape)\n",
    "\n",
    "nb_classifier = BernoulliNB()\n",
    "nb_classifier.fit(X_train_BOW_resampled_ADA, y_train_BOW_resampled_ADA)\n",
    "\n",
    "y_pred_BOW_ADA = nb_classifier.predict(X_test_BOW)\n",
    "accuracy = accuracy_score(y_test, y_pred_BOW_ADA)\n",
    "accuracy_df.loc['ADASYN', 'BOW'] = accuracy\n",
    "\n",
    "print(accuracy_df)\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train_TFIDF_resampled_ADA, y_train_TFIDF_resampled_ADA)\n",
    "\n",
    "y_pred_svm_ADA = svm_classifier.predict(X_test_TFIDF)\n",
    "accuracy_svm_ADA = accuracy_score(y_test, y_pred_svm_ADA)\n",
    "print(\"SVM Accuracy:\", accuracy_svm_ADA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95609da3-2bac-42eb-be2f-e4eac4ca66d1",
   "metadata": {},
   "source": [
    "## **Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d909a247-5e68-48d1-8623-069457cb4a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "data = pd.concat([domain1, domain2], ignore_index=True)\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "sentences_list = [nltk.word_tokenize(text) for text in data['text']]\n",
    "y = data['label'].values\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences_list, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "def document_vector(model, doc):\n",
    "    doc = [word for word in doc if word in model.wv.key_to_index]\n",
    "    return np.mean(model.wv[doc], axis=0) if doc else np.zeros(model.vector_size)\n",
    "    \n",
    "test_sentences_list = [nltk.word_tokenize(text) for text in test_data_df['text']]\n",
    "feature_vectors = np.array([document_vector(model, doc) for doc in sentences_list])\n",
    "test_feature_vectors = np.array([document_vector(model, doc) for doc in test_sentences_list])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_W2V, X_test_W2V, y_train, y_test = train_test_split(feature_vectors, y, test_size=0.2, random_state=42)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=35)\n",
    "X_train_W2V_resampled, y_train_W2V_resampled = smote.fit_resample(X_train_W2V, y_train)\n",
    "from sklearn.svm import SVC\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train_W2V_resampled, y_train_W2V_resampled)\n",
    "y_pred_W2V = svm_classifier.predict(X_test_W2V)\n",
    "y_pred_test = svm_classifier.predict(test_feature_vectors)\n",
    "accuracy_W2V = accuracy_score(y_test, y_pred_W2V)\n",
    "print(accuracy_W2V)\n",
    "results_csv = pd.DataFrame({\n",
    "        \"id\": id,\n",
    "        \"class\":y_pred_test,\n",
    "    })\n",
    "results_csv.to_csv('test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ada46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tf.reset_default_graph()\n",
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 50000\n",
    "numDimensions = 300\n",
    "maxSeqLength = 250 #Maximum length of sentence\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "data = pd.concat([domain1, domain2], ignore_index=True)\n",
    "max_length = 250\n",
    "sequences_padded = pad_sequences(test_data_df['text'], maxlen=max_length, padding='post', truncating='post', value=0)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda x: list(map(int, x.split())))\n",
    "\n",
    "\n",
    "max_length = 250\n",
    "sequences_padded = pad_sequences(train_data['text'], maxlen=max_length, padding='post', truncating='post', value=0)\n",
    "\n",
    "indices = np.random.permutation(len(sequences_padded))\n",
    "texts_shuffled = sequences_padded[indices]\n",
    "labels_shuffled = train_data['label'].values[indices]\n",
    "\n",
    "batch_size = 24\n",
    "num_batches = len(texts_shuffled) // batch_size\n",
    "text_batches = [texts_shuffled[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)]\n",
    "label_batches = [labels_shuffled[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)]\n",
    "\n",
    "if len(texts_shuffled) % batch_size != 0:\n",
    "    extra_texts = texts_shuffled[num_batches * batch_size:]\n",
    "    extra_labels = labels_shuffled[num_batches * batch_size:]\n",
    "    text_batches.append(extra_texts)\n",
    "    label_batches.append(extra_labels)\n",
    "def get_random_batch(text_batches, label_batches):\n",
    "    batch_index = np.random.randint(len(text_batches))\n",
    "    return text_batches[batch_index], label_batches[batch_index]\n",
    "\n",
    "\n",
    "test_data ['text'] = test_data ['text'].apply(lambda x: list(map(int, x.split())))\n",
    "\n",
    "\n",
    "max_length = 250\n",
    "sequences_padded1 = pad_sequences(test_data ['text'], maxlen=max_length, padding='post', truncating='post', value=0)\n",
    "\n",
    "indices1 = np.random.permutation(len(sequences_padded1))\n",
    "texts_shuffled1 = sequences_padded[indices1]\n",
    "labels_shuffled1 = train_data['label'].values[indices1]\n",
    "\n",
    "batch_size = 24\n",
    "num_batches1 = len(texts_shuffled1) // batch_size\n",
    "text_batches1 = [texts_shuffled1[i * batch_size:(i + 1) * batch_size] for i in range(num_batches1)]\n",
    "label_batches1 = [labels_shuffled1[i * batch_size:(i + 1) * batch_size] for i in range(num_batches1)]\n",
    "\n",
    "if len(texts_shuffled1) % batch_size != 0:\n",
    "    extra_texts1 = texts_shuffled1[num_batches1 * batch_size:]\n",
    "    extra_labels1 = labels_shuffled1[num_batches1 * batch_size:]\n",
    "    text_batches1.append(extra_texts1)\n",
    "    label_batches1.append(extra_labels1)\n",
    "def get_test_batch(text_batches1, label_batches1):\n",
    "    batch_index1 = np.random.randint(len(text_batches1))\n",
    "    return text_batches1[batch_index1], label_batches1[batch_index1]\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 20001\n",
    "numDimensions = 300\n",
    "maxSeqLength = 250  # \n",
    "\n",
    "embedding_matrix = model.wv.vectors\n",
    "embedding_tensor = tf.Variable(initial_value=embedding_matrix, trainable=False, dtype=tf.float32)\n",
    "\n",
    "\n",
    "input_data = tf.placeholder(tf.int32, [None, maxSeqLength])\n",
    "labels = tf.placeholder(tf.float32, [None, numClasses])\n",
    "\n",
    "\n",
    "data = tf.nn.embedding_lookup(embedding_tensor, input_data)\n",
    "\n",
    "\n",
    "lstmCell = tf.nn.rnn_cell.BasicLSTMCell(lstmUnits, reuse=tf.AUTO_REUSE)\n",
    "lstmCell = tf.nn.rnn_cell.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "outputs, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(outputs, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = tf.matmul(last, weight) + bias\n",
    "correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = get_random_batch(text_batches, label_batches)\n",
    "    \n",
    "    # Convert labels to one-hot encoding\n",
    "    nextBatchLabels_one_hot = tf.one_hot(nextBatchLabels, depth=numClasses)\n",
    "    nextBatchLabels_one_hot = sess.run(nextBatchLabels_one_hot)  # evaluate the one-hot tensor\n",
    "    \n",
    "    # Run the optimization\n",
    "    sess.run(optimizer, feed_dict={input_data: nextBatch, labels: nextBatchLabels_one_hot})\n",
    "    \n",
    "    if (i % 1000 == 0 and i != 0):\n",
    "        loss_ = sess.run(loss, feed_dict={input_data: nextBatch, labels: nextBatchLabels_one_hot})\n",
    "        accuracy_ = sess.run(accuracy, feed_dict={input_data: nextBatch, labels: nextBatchLabels_one_hot})\n",
    "        \n",
    "        print(\"iteration {}/{}...\".format(i+1, iterations),\n",
    "              \"loss {}...\".format(loss_),\n",
    "              \"accuracy {}...\".format(accuracy_))\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = get_test_batch(text_batches1, label_batches1)\n",
    "    nextBatchLabels_one_hot = np.eye(numClasses)[nextBatchLabels]\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels_one_hot})) * 100)\n",
    "\n",
    "predicted_probabilities = sess.run(prediction, feed_dict={input_data: sequences_padded0})\n",
    "predicted_labels = np.argmax(predicted_probabilities, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "print(len(predicted_labels))\n",
    "id_list = range(len(predicted_labels))\n",
    "result_df = pd.DataFrame({\"id\": id_list, \"predicted_labels\": predicted_labels})\n",
    "result_df.to_csv(\"predicted_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390e4f7-e77c-4c6a-a57e-356268e9d5c6",
   "metadata": {},
   "source": [
    "## **DaNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e038ec2-713a-43c3-af2e-5d995787eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self, feature_dim=len(vectorizer.vocabulary_)):\n",
    "        super(Extractor, self).__init__()\n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Linear(in_features=feature_dim, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.extractor(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=1),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_feature, alpha):\n",
    "        reversed_input = ReverseLayerF.apply(input_feature, alpha)\n",
    "        x = self.discriminator(reversed_input)\n",
    "        return x\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05388a89-2194-4634-a5de-77ff894a45f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Total Loss: 42.52740478515625, Label Loss: 41.89863586425781, Domain Loss: 0.6287685632705688\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(domain1['text'], domain1['label'].values, test_size=0.2, random_state=42, stratify=domain1['label'].values)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(domain2['text'], domain2['label'].values, test_size=0.2, random_state=42, stratify=domain2['label'].values)\n",
    "\n",
    "combined_X_train = pd.concat([X_train_1, X_train_2])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(combined_X_train)\n",
    "\n",
    "X_train_1_TFIDF = vectorizer.transform(X_train_1)\n",
    "X_train_2_TFIDF = vectorizer.transform(X_train_2)\n",
    "\n",
    "X_train_2_TFIDF, y_train_2 = ADASYN(sampling_strategy='auto', random_state=35, n_neighbors=5, n_jobs=None).fit_resample(X_train_2_TFIDF, y_train_2)\n",
    "\n",
    "X_train_1 = torch.from_numpy(X_train_1_TFIDF.toarray())\n",
    "y_train_1 = torch.from_numpy(y_train_1).to(dtype=torch.float)\n",
    "X_train_2 = torch.from_numpy(X_train_2_TFIDF.toarray())\n",
    "y_train_2 = torch.from_numpy(y_train_2).to(dtype=torch.float)\n",
    "\n",
    "# Create iterable dataset in Torch format\n",
    "train_1_ds = torch.utils.data.TensorDataset(X_train_1, y_train_1)\n",
    "train_1_loader = torch.utils.data.DataLoader(train_1_ds, batch_size=32)\n",
    "train_2_ds = torch.utils.data.TensorDataset(X_train_2, y_train_2)\n",
    "train_2_loader = torch.utils.data.DataLoader(train_2_ds, batch_size=32)\n",
    "\n",
    "\n",
    "feature_extractor = Extractor(feature_dim=len(vectorizer.vocabulary_))\n",
    "label_classifier = Classifier()\n",
    "domain_classifier = Discriminator()\n",
    "\n",
    "label_classification_criterion = nn.CrossEntropyLoss()\n",
    "domain_classification_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "DaNN_params = list(feature_extractor.parameters()) + list(label_classifier.parameters()) + list(domain_classifier.parameters())\n",
    "DaNN_optimizer = optim.Adam(DaNN_params, lr=0.001)\n",
    "\n",
    "alpha = 0.5\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    feature_extractor.train()\n",
    "    label_classifier.train()\n",
    "    domain_classifier.train()\n",
    "    for i, (data_1, data_2) in enumerate(zip(train_1_loader, train_2_loader)):\n",
    "        text_1, label_1 = data_1\n",
    "        text_2, label_2 = data_2\n",
    "        combined_text = torch.cat((text_1, text_2), 0)\n",
    "        \n",
    "        feature_1 = feature_extractor(text_1)\n",
    "        feature_2 = feature_extractor(text_2)\n",
    "        combined_feature = feature_extractor(combined_text)\n",
    "        \n",
    "        label_prediction_1 = label_classifier(feature_1)\n",
    "        label_prediction_2 = label_classifier(feature_2)\n",
    "        # print(label_prediction_1)\n",
    "        # print(label_1)\n",
    "        label_loss_1 = label_classification_criterion(label_prediction_1, label_1)\n",
    "        label_loss_2 = label_classification_criterion(label_prediction_2, label_2)\n",
    "\n",
    "        domain_prediction = domain_classifier(combined_feature, alpha)\n",
    "        domain_combined_label = torch.cat((torch.zeros(text_1.shape[0]).long(), torch.ones(text_2.shape[0]).long()), 0)\n",
    "\n",
    "        domain_loss = domain_classification_criterion(domain_prediction, domain_combined_label)\n",
    "        \n",
    "        total_loss = label_loss_1 + label_loss_2 + domain_loss\n",
    "        DaNN_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        DaNN_optimizer.step()\n",
    "        \n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}, Total Loss: {total_loss.item()}, Label Loss: {(label_loss_1+label_loss_2).item()}, Domain Loss: {domain_loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5babe3-5c00-4efb-abc4-439ffb5b8187",
   "metadata": {},
   "source": [
    "## **Test Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4266a2b-c8fc-4f6d-853d-4a1da4ea8eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  class\n",
      "0        0      1\n",
      "1        1      0\n",
      "2        2      0\n",
      "3        3      0\n",
      "4        4      0\n",
      "...    ...    ...\n",
      "3995  3995      0\n",
      "3996  3996      0\n",
      "3997  3997      0\n",
      "3998  3998      0\n",
      "3999  3999      1\n",
      "\n",
      "[4000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_predict_csv(clfs, X_test, id):\n",
    "    \n",
    "    prediction = clfs.predict(X_test)\n",
    "    results_csv = pd.DataFrame({\n",
    "        \"id\": id,\n",
    "        \"class\": prediction,\n",
    "    })\n",
    "    return results_csv\n",
    "\n",
    "test_results = get_predict_csv(svm_classifier, X_test_oversampling, test_data_df[\"id\"])\n",
    "print(test_results)\n",
    "test_results.to_csv('test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff847852-3b79-41a4-808f-e52b561736db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "839bb10b-6318-471a-bcff-fae1655867da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10, Total Loss: 108765.1796875, Label Loss: 108764.5390625, Domain Loss: 0.6370952129364014\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20, Total Loss: 104948.5234375, Label Loss: 104947.953125, Domain Loss: 0.573634684085846\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30, Total Loss: 102620.3515625, Label Loss: 102619.859375, Domain Loss: 0.49597883224487305\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40, Total Loss: 102241.40625, Label Loss: 102240.921875, Domain Loss: 0.4878908395767212\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m label_loss_1 \u001b[38;5;241m+\u001b[39m label_loss_2 \u001b[38;5;241m+\u001b[39m domain_loss\n\u001b[1;32m     64\u001b[0m DaNN_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 65\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     66\u001b[0m DaNN_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(domain1['text'], domain1['label'].values, test_size=0.2, random_state=42, stratify=domain1['label'].values)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(domain2['text'], domain2['label'].values, test_size=0.2, random_state=42, stratify=domain2['label'].values)\n",
    "\n",
    "combined_X_train = pd.concat([X_train_1, X_train_2])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(combined_X_train)\n",
    "\n",
    "X_train_1_TFIDF = vectorizer.transform(X_train_1)\n",
    "X_train_2_TFIDF = vectorizer.transform(X_train_2)\n",
    "\n",
    "X_train_2_TFIDF, y_train_2 = ADASYN(sampling_strategy='auto', random_state=35, n_neighbors=5, n_jobs=None).fit_resample(X_train_2_TFIDF, y_train_2)\n",
    "\n",
    "X_train_1 = torch.from_numpy(X_train_1_TFIDF.toarray()).to(dtype=torch.float)\n",
    "y_train_1 = torch.from_numpy(y_train_1).to(dtype=torch.float)\n",
    "X_train_2 = torch.from_numpy(X_train_2_TFIDF.toarray()).to(dtype=torch.float)\n",
    "y_train_2 = torch.from_numpy(y_train_2).to(dtype=torch.float)\n",
    "\n",
    "# Create iterable dataset in Torch format\n",
    "train_1_ds = torch.utils.data.TensorDataset(X_train_1, y_train_1)\n",
    "train_1_loader = torch.utils.data.DataLoader(train_1_ds, batch_size=32)\n",
    "train_2_ds = torch.utils.data.TensorDataset(X_train_2, y_train_2)\n",
    "train_2_loader = torch.utils.data.DataLoader(train_2_ds, batch_size=32)\n",
    "\n",
    "\n",
    "feature_extractor = Extractor(feature_dim=len(vectorizer.vocabulary_))\n",
    "label_classifier = Classifier()\n",
    "domain_classifier = Discriminator()\n",
    "\n",
    "label_classification_criterion = nn.CrossEntropyLoss()\n",
    "domain_classification_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "DaNN_params = list(feature_extractor.parameters()) + list(label_classifier.parameters()) + list(domain_classifier.parameters())\n",
    "DaNN_optimizer = optim.Adam(DaNN_params, lr=0.001)\n",
    "\n",
    "alpha = 0.5\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    \n",
    "    feature_extractor.train()\n",
    "    label_classifier.train()\n",
    "    domain_classifier.train()\n",
    "\n",
    "    feature_1 = feature_extractor(X_train_1)\n",
    "    feature_2 = feature_extractor(X_train_2)\n",
    "    X_train_combined = torch.cat((X_train_1, X_train_2), 0)\n",
    "    combined_feature = feature_extractor(X_train_combined)\n",
    "\n",
    "    label_prediction_1 = label_classifier(feature_1)\n",
    "    label_prediction_2 = label_classifier(feature_2)\n",
    "    label_loss_1 = label_classification_criterion(label_prediction_1, y_train_1)\n",
    "    label_loss_2 = label_classification_criterion(label_prediction_2, y_train_2)\n",
    "    \n",
    "    domain_prediction = domain_classifier(combined_feature, alpha)\n",
    "    domain_combined_label = torch.cat((torch.zeros(X_train_1.shape[0]).long(), torch.ones(X_train_2.shape[0]).long()), 0)\n",
    "    domain_loss = domain_classification_criterion(domain_prediction, domain_combined_label)\n",
    "        \n",
    "    total_loss = label_loss_1 + label_loss_2 + domain_loss\n",
    "    DaNN_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    DaNN_optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch: {epoch+1}, Total Loss: {total_loss.item()}, Label Loss: {(label_loss_1+label_loss_2).item()}, Domain Loss: {domain_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306c7c3-1b56-4aaf-ba3f-49133b85fbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
